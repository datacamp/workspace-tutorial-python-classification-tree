{"metadata":{"language_info":{"name":""},"kernelspec":{"name":"","display_name":""}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Supervised Learningâ€”How to do a Classification Tree in Python","metadata":{},"id":"4be58857-7ab7-4038-89fa-967805dbee1e"},{"cell_type":"markdown","source":"## When to use a classification tree","metadata":{},"id":"7b9d1ed7-b3d7-4f8e-b2d2-1b8783db5ea2"},{"cell_type":"markdown","source":"- When the response variable (the thing you are trying to predict) is a logical (true or false) or categorical variable.\n- You need easy interpretability of the results.\n- You want a computationally cheap model.\n- You want to convert continuous features into categorical features as a feature engineeering step for a deep learning model.","metadata":{},"id":"29bb2b6c-8368-4acc-aef8-b302b516b99c"},{"cell_type":"markdown","source":"## Do you actually want an ensemble of classification trees?","metadata":{},"id":"297a6b12-adcf-48fd-947c-39107bfd2c8e"},{"cell_type":"markdown","source":"Classification trees are very simple models which often have weak predictive power. In most cases, you need to increase their power and robustness by using an ensemble of trees. That means:\n\n- a bagging method like random forests, or\n- a boosting method like gradient boosting.","metadata":{},"id":"3f87e7f1-993d-4eba-8bb5-e3ff4f9fade0"},{"cell_type":"markdown","source":"## Which Python packages can you use?","metadata":{},"id":"b582eb8d-b56f-4d7c-aef3-ab4fadbe00c3"},{"cell_type":"markdown","source":"- scikit learn (used here)\n- PyCaret","metadata":{},"id":"7fa44c3c-1c91-41a5-b517-4d5e5e8115c3"},{"cell_type":"markdown","source":"## Case study: determining the variety of raisin","metadata":{"tags":[]},"id":"f7d08841-ca83-43a1-993b-7c8eabd88526"},{"cell_type":"markdown","source":"Automated food detection is useful in food production, food safety, and dietary monitoring. \n\nThe process traditionally has two steps:\n\n1. Convert images of food into numeric features, like dimensions, shape, color.\n2. Run a classification model on those features.\n\nThis raisin dataset, sourced from the [UCI Machine Learning Archive](https://archive.ics.uci.edu/ml/datasets/Raisin+Dataset) contains the output of step 1 for varieties of Turkish raisin.\n\nWe'll need **pandas** for importing the data and doing some manipulation, then **scikit-learn** for modeling, and **matplotlib** for plotting.","metadata":{},"id":"0d93c88e-ce95-4933-958b-c00ebd462b3d"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"f962b80f-c09f-47c4-9921-b56e12fb7bd0"},{"cell_type":"markdown","source":"The dataset is in a CSV file named `\"raisins.csv\"`.","metadata":{},"id":"8d317862-d800-40e4-a3be-b9493c4b93e4"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"c1ce6f7d-9070-43ff-8def-5e6d44fcec92"},{"cell_type":"markdown","source":"## Data dictionary","metadata":{},"id":"d0530534-727d-4a0b-b287-bcf688619923"},{"cell_type":"markdown","source":"Each row represents one raisin. Some columns are easier to interpret if you think of the raisin as being an ellipse.\n\n![](ellipse.png)\n\n- **Area**: Area of the raisin in pixels. Approximately `pi * a * b`.\n- **MajorAxisLength**: The length of the longest diameter of the raisin in pixels. Equal to `2 * b`.\n- **MinorAxisLength**: The length of the shortest diameter of the raisin in pixels. Equal to `2 * a`.\n- **Eccentricity**: How close to circular is the raisin? Equal to `sqrt(1 - (2 * a) ^ 2 / (2 * b) ** 2)`.\n- **ConvexArea**: Area of smallest convex shape around the raisin in pixel. Approximately `pi * a * b`, and slightly more than Area.\n- **Extent**: Fraction of a rectangle drawn around the raisin that contains the raisin image.\n- **Perimeter**: Perimeter of the raisin in pixels. Approximately `pi * (3 * (a + b) - sqrt((3 * a + b) * (a + 3 * b)))`\n- **Variety**: The variety of raisin. Either **Kecimen** (sour black grape) or **Besni** (pale grape).","metadata":{},"id":"80ecf77a-b5a5-4823-853c-f200e8f6ad3c"},{"cell_type":"markdown","source":"## Splitting into response and explanatory columns","metadata":{},"id":"fabed788-dc3c-451a-977f-99225dfbfc03"},{"cell_type":"markdown","source":"The response column is `\"Variety\"`. The explanatory (input) columns are all the other columns.","metadata":{},"id":"1f783c76-6966-4791-9c58-2ee67c3b7bf3"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"31420b15-39a6-473b-b939-3fbfe8082ebf"},{"cell_type":"markdown","source":"## Splitting into training and testing sets","metadata":{},"id":"d9c208df-bb6b-4b44-8391-92cfd6d137fd"},{"cell_type":"markdown","source":"The explanatory and response datasets need to be split into training and testing sets. \n\nHere we'll use [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with the default arguments.","metadata":{},"id":"18ed0914-04e4-446a-8a2d-ae976ad894b8"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"9951d017-bd0d-4ac8-b786-aa2707dd0760"},{"cell_type":"markdown","source":"## Fitting the model to the training set","metadata":{},"id":"e70e2815-3057-48c3-919b-09109d699eb0"},{"cell_type":"markdown","source":"The data is now ready to model. The first modeling step is to create a [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) object.","metadata":{},"id":"1cfefaa2-9496-4af0-a362-a815673c9472"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"4d497352-7a31-4325-b0fd-df58d6049fa2"},{"cell_type":"markdown","source":"Use the [`.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit) method to fit the model to the training set.","metadata":{},"id":"fb283446-6bca-4ec6-be58-1ff8acf8fd61"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"b2cd887a-3d36-4b58-8204-5ec2243593ed"},{"cell_type":"markdown","source":"## Making predictions on the testing set","metadata":{},"id":"7adef66c-eb05-4a3c-8b02-db4ae4099965"},{"cell_type":"markdown","source":"You can calculate the predicted response with the [`.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict) method.","metadata":{},"id":"5d1bbf65-7aca-49df-b03b-abf92081ea19"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"7676b9d0-4c3a-44bc-8827-141ea2162acb"},{"cell_type":"markdown","source":"## Assessing model performance","metadata":{},"id":"34fdc9e4-3c81-49a5-87c8-e2e9a3be09e5"},{"cell_type":"markdown","source":"There are four possible outcomes, depending on whether the actual response and the predicted response are true or false. The confusion matrix, created with [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) shows the counts of each case.\n\n|                     |**predicted Besni** |**predicted Kecimen** |\n|:--------------------|:-----------------|:----------------|\n|**actual Besni** |correct           |false positive   |\n|**actual Kecimen**  |false negative    |correct          |","metadata":{},"id":"41e2e91e-9500-4709-bf52-e4d7166f4423"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"772ed3f1-3678-4049-8d6b-48d373c01403"},{"cell_type":"markdown","source":"[`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) provides a commonly used metric about the performance of the model. \n\n**Accuracy**: What fraction of the values were correctly predicted? (Sum of diagonal divided by sum of all values.)","metadata":{},"id":"6d84845c-d90f-4efa-872d-09df33defb8a"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"4f4a2b29-164b-423b-bc7a-fa2b8ac40762"},{"cell_type":"markdown","source":"Visualizing the tree helps to see how the decisions are made. Using [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) is the simplest way to do this.","metadata":{},"id":"331371ff-4e64-4958-97c8-fc2a835126fc"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"f15a0fa3-9b96-41d8-91db-755369144f30"},{"cell_type":"markdown","source":"## Want to learn more?","metadata":{},"id":"095674ec-5cf7-43d6-ba95-0a838b2a1e7d"},{"cell_type":"markdown","source":"- The scikit learn tutorial on [Understanding the decision tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) provides more details on how to underastand the resulting tree.\n- Several DataCamp courses cover decision trees with scikit-learn. Starts with [Machine Learning with Tree-Based Models in Python](https://app.datacamp.com/learn/courses/machine-learning-with-tree-based-models-in-python), and consider industry-specific variants like [Machine Learning for Finance in Python](https://app.datacamp.com/learn/courses/machine-learning-for-finance-in-python) and [Machine Learning for Marketing in Python](https://app.datacamp.com/learn/courses/machine-learning-for-marketing-in-python).\n- Try applying your skills with this [Decision Tree Classification](https://app.datacamp.com/workspace/templates/playbook-python-decision-tree-marketing-data) Workspace template.","metadata":{},"id":"af1c4076-e5df-4ef1-8f8f-bad4eb239459"}]}